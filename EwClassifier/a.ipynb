{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model found. Training from scratch.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 5, 64)             17920     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30501 (119.14 KB)\n",
      "Trainable params: 30501 (119.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "input_shape=(data.trainX.shape[1], data.trainX.shape[2]) 5 5\n",
      "trainX [[[   167.      172.      165.      171.    80000.  ]\n",
      "  [   150.      155.      148.      152.   120000.  ]\n",
      "  [   160.5     166.4     159.6     164.5   95000.  ]\n",
      "  [   158.75    159.5     156.5     157.5  110000.  ]\n",
      "  [   175.      180.      173.      179.    85000.  ]]\n",
      "\n",
      " [[   150.      155.      148.      152.   120000.  ]\n",
      "  [   160.5     166.4     159.6     164.5   95000.  ]\n",
      "  [   158.75    159.5     156.5     157.5  110000.  ]\n",
      "  [   175.      180.      173.      179.    85000.  ]\n",
      "  [   157.      162.      155.      160.5  105000.  ]]\n",
      "\n",
      " [[   160.5     166.4     159.6     164.5   95000.  ]\n",
      "  [   158.75    159.5     156.5     157.5  110000.  ]\n",
      "  [   175.      180.      173.      179.    85000.  ]\n",
      "  [   157.      162.      155.      160.5  105000.  ]\n",
      "  [   178.      183.      176.      182.    80000.  ]]\n",
      "\n",
      " [[   158.75    159.5     156.5     157.5  110000.  ]\n",
      "  [   175.      180.      173.      179.    85000.  ]\n",
      "  [   157.      162.      155.      160.5  105000.  ]\n",
      "  [   178.      183.      176.      182.    80000.  ]\n",
      "  [   171.74    177.18    169.82    175.68  80000.  ]]\n",
      "\n",
      " [[   175.      180.      173.      179.    85000.  ]\n",
      "  [   157.      162.      155.      160.5  105000.  ]\n",
      "  [   178.      183.      176.      182.    80000.  ]\n",
      "  [   171.74    177.18    169.82    175.68  80000.  ]\n",
      "  [   172.      177.      170.      176.    95000.  ]]\n",
      "\n",
      " [[   157.      162.      155.      160.5  105000.  ]\n",
      "  [   178.      183.      176.      182.    80000.  ]\n",
      "  [   171.74    177.18    169.82    175.68  80000.  ]\n",
      "  [   172.      177.      170.      176.    95000.  ]\n",
      "  [   160.      160.75    157.75    158.75 105000.  ]]\n",
      "\n",
      " [[   178.      183.      176.      182.    80000.  ]\n",
      "  [   171.74    177.18    169.82    175.68  80000.  ]\n",
      "  [   172.      177.      170.      176.    95000.  ]\n",
      "  [   160.      160.75    157.75    158.75 105000.  ]\n",
      "  [   161.25    162.      159.      160.    90000.  ]]\n",
      "\n",
      " [[   171.74    177.18    169.82    175.68  80000.  ]\n",
      "  [   172.      177.      170.      176.    95000.  ]\n",
      "  [   160.      160.75    157.75    158.75 105000.  ]\n",
      "  [   161.25    162.      159.      160.    90000.  ]\n",
      "  [   160.      164.      158.      162.    95000.  ]]\n",
      "\n",
      " [[   172.      177.      170.      176.    95000.  ]\n",
      "  [   160.      160.75    157.75    158.75 105000.  ]\n",
      "  [   161.25    162.      159.      160.    90000.  ]\n",
      "  [   160.      164.      158.      162.    95000.  ]\n",
      "  [   169.      174.      167.      173.5   85000.  ]]\n",
      "\n",
      " [[   160.      160.75    157.75    158.75 105000.  ]\n",
      "  [   161.25    162.      159.      160.    90000.  ]\n",
      "  [   160.      164.      158.      162.    95000.  ]\n",
      "  [   169.      174.      167.      173.5   85000.  ]\n",
      "  [   157.5     158.25    155.25    156.25 100000.  ]]\n",
      "\n",
      " [[   161.25    162.      159.      160.    90000.  ]\n",
      "  [   160.      164.      158.      162.    95000.  ]\n",
      "  [   169.      174.      167.      173.5   85000.  ]\n",
      "  [   157.5     158.25    155.25    156.25 100000.  ]\n",
      "  [   156.      160.      154.      158.   110000.  ]]\n",
      "\n",
      " [[   160.      164.      158.      162.    95000.  ]\n",
      "  [   169.      174.      167.      173.5   85000.  ]\n",
      "  [   157.5     158.25    155.25    156.25 100000.  ]\n",
      "  [   156.      160.      154.      158.   110000.  ]\n",
      "  [   181.      186.      179.      185.    75000.  ]]\n",
      "\n",
      " [[   169.      174.      167.      173.5   85000.  ]\n",
      "  [   157.5     158.25    155.25    156.25 100000.  ]\n",
      "  [   156.      160.      154.      158.   110000.  ]\n",
      "  [   181.      186.      179.      185.    75000.  ]\n",
      "  [   155.      160.      153.      158.    90000.  ]]\n",
      "\n",
      " [[   157.5     158.25    155.25    156.25 100000.  ]\n",
      "  [   156.      160.      154.      158.   110000.  ]\n",
      "  [   181.      186.      179.      185.    75000.  ]\n",
      "  [   155.      160.      153.      158.    90000.  ]\n",
      "  [   162.5     163.25    160.5     161.25  85000.  ]]\n",
      "\n",
      " [[   156.      160.      154.      158.   110000.  ]\n",
      "  [   181.      186.      179.      185.    75000.  ]\n",
      "  [   155.      160.      153.      158.    90000.  ]\n",
      "  [   162.5     163.25    160.5     161.25  85000.  ]\n",
      "  [   175.      180.      173.      178.5  100000.  ]]]\n",
      "trainY [[0 0 0 0 1]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 1 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 1 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 1 0]]\n",
      "Epoch 1/20\n",
      "13/13 [==============================] - 2s 25ms/step - loss: 6.5053 - accuracy: 0.0769 - val_loss: 0.8098 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 6.5337 - accuracy: 0.2308 - val_loss: 8.0590 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 8.8542 - accuracy: 0.4615 - val_loss: 0.7266 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 6.5206 - accuracy: 0.1538 - val_loss: 0.2401 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 6.4268 - accuracy: 0.1538 - val_loss: 1.1921e-07 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 8.8321 - accuracy: 0.1538 - val_loss: 16.1181 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 8.7617 - accuracy: 0.2308 - val_loss: 1.1921e-07 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 7.6437 - accuracy: 0.2308 - val_loss: 1.1921e-07 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 7.5850 - accuracy: 0.2308 - val_loss: 8.4193 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 5.7164 - accuracy: 0.1538 - val_loss: 8.3917 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 3.3745 - accuracy: 0.0000e+00 - val_loss: 8.5282 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.8669 - accuracy: 0.2308 - val_loss: 8.0590 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 5.0628 - accuracy: 0.3077 - val_loss: 8.0590 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 5.2577 - accuracy: 0.1538 - val_loss: 8.0590 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 11.3451 - accuracy: 0.3077 - val_loss: 0.3342 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 5.1953 - accuracy: 0.1538 - val_loss: 0.3769 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 7.4841 - accuracy: 0.2308 - val_loss: 0.4912 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 7.8204 - accuracy: 0.3077 - val_loss: 1.1921e-07 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 8.7910 - accuracy: 0.3077 - val_loss: 1.1921e-07 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 9.9200 - accuracy: 0.0769 - val_loss: 8.0590 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: a_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: a_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history <keras.src.callbacks.History object at 0x00000218D4BB1E50>\n",
      "1/1 [==============================] - 0s 221ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "import os\n",
    "import shutil\n",
    "from utils_plot import plot_candlestick_chart\n",
    "from predictProcessor import PredictProcessor\n",
    "from inputProcessor import InputProcessor\n",
    "\n",
    "# Define the path to the data folder\n",
    "data_folder = 'data/learn'\n",
    "output_folder = 'training_plots'  # Specify the output folder\n",
    "\n",
    "# Clean the output folder before each run\n",
    "if os.path.exists(output_folder):\n",
    "    shutil.rmtree(output_folder)  # Remove the folder and its contents\n",
    "\n",
    "# Create the output folder\n",
    "os.makedirs(output_folder)\n",
    "\n",
    "# Get a list of all files in the data folder\n",
    "file_list = [filename for filename in os.listdir(data_folder) if filename.endswith('.csv')]\n",
    "\n",
    "# Loop through each file\n",
    "for filename in file_list:\n",
    "    file_path = os.path.join(data_folder, filename)\n",
    "    stock_name = os.path.splitext(filename)[0]\n",
    "    data = InputProcessor(file_path)\n",
    "\n",
    "    # Load model weights if available\n",
    "    try:\n",
    "        model = load_model('a_model')\n",
    "        print(\"Model loaded successfully.\")\n",
    "    except:\n",
    "        print(\"No model found. Training from scratch.\")\n",
    "        # Build the LSTM model\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(64, activation='relu', input_shape=(data.trainX.shape[1], data.trainX.shape[2]), return_sequences=True))\n",
    "        model.add(LSTM(32, activation='relu', return_sequences=False))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=5))  # Output layer for 5 classes (Wave 1 through 5)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        \"\"\" \n",
    "        model = Sequential()\n",
    "        model.add(LSTM(128, activation='relu', input_shape=(\n",
    "                    data.trainX.shape[1], data.trainX.shape[2]), return_sequences=True))\n",
    "        model.add(LSTM(64, activation='relu', return_sequences=False))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \"\"\"\n",
    "\n",
    "        \"\"\" model = Sequential()\n",
    "        model.add(LSTM(64, activation='relu', input_shape=(\n",
    "                    data.trainX.shape[1], data.trainX.shape[2]), return_sequences=True))\n",
    "        model.add(LSTM(32, activation='relu', return_sequences=False))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=1, activation='sigmoid'))  # Output layer with 1 units for Wave category\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \"\"\"\n",
    "\n",
    "        \"\"\"     model = Sequential()\n",
    "        model.add(LSTM(units=64, activation='relu', input_shape=(data.n_past, 5)))  # Adjust input_shape here\n",
    "        model.add(Dense(units=32, activation='relu'))\n",
    "        model.add(Dense(units=5, activation='sigmoid'))  # Output layer with 5 units for Wave1 to Wave5\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \"\"\"\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    print(\"input_shape=(data.trainX.shape[1], data.trainX.shape[2])\", data.trainX.shape[1], data.trainX.shape[2])\n",
    "    print(\"trainX\", data.trainX)\n",
    "    print(\"trainY\", data.trainY)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(data.trainX, data.trainY, epochs=20, batch_size=1, validation_split=0.1, verbose=1)\n",
    "    # fit the model\n",
    "    model.save('a_model')\n",
    "\n",
    "\n",
    "    print('history', history)\n",
    "    # Save the plot as an image file in the output folder\n",
    "    #output_filename = os.path.join(output_folder, os.path.splitext(filename)[0] + '_plot.png')\n",
    "    #print_plot_model_loss(output_filename, history.history['loss'], history.history['val_loss'])\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    #result = model.evaluate(X_test_sequences, y_test_sequences, verbose=1)\n",
    "    #print(\"Evaluation result:\", result)\n",
    "    #loss, accuracy = model.evaluate(X_test_sequences, y_test_sequences, verbose=1)\n",
    "    #print(f'Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}')\n",
    "\n",
    "    PredictProcessor(model, data, stock_name, output_folder)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
